{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"provenance":[],"gpuType":"V28"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9-xS6WWYwH9J"},"source":["# Vision Transformer (ViT) with Pizza, Steak, Sushi Dataset\n","\n","이 노트북은 **Vision Transformer(ViT)** 논문 구조를 간략히 구현하고, `pizza_steak_sushi` 데이터셋을 이용해 분류 문제를 학습합니다.\n","\n","## 진행 순서\n","1. **환경 설정** 및 필요한 함수 다운로드\n","2. **데이터 다운로드** (`pizza_steak_sushi`)\n","3. **Dataset & DataLoader** 만들기\n","4. **ViT 모델 구현** (Patch Embedding, MSA, MLP, Transformer Encoders)\n","5. **학습 루프** 정의\n","6. **학습 실행** 및 결과 확인"],"id":"9-xS6WWYwH9J"},{"cell_type":"code","metadata":{"id":"environment-setup"},"source":["%%capture\n","# Colab 환경에서 torch, torchvision이 없다면 설치 (대부분은 기본 포함)\n","!pip install torch torchvision\n","\n","# helper_functions.py 다운로드 (여기에 download_data 함수가 들어있음)\n","try:\n","    from helper_functions import download_data\n","except ImportError:\n","    !wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\n","    from helper_functions import download_data\n","\n","# 위 명령 결과 출력이 길 수 있으니 %%capture로 숨김.\n","# 실행이 끝나면 아래 셀로 넘어가세요."],"execution_count":null,"outputs":[],"id":"environment-setup"},{"cell_type":"markdown","metadata":{"id":"YNRsyJj2wH9L"},"source":["## 1. 환경 설정\n","\n","아래 셀을 통해 **PyTorch**와 **TorchVision**이 설치되었는지 확인하고, GPU를 사용 가능한지 체크합니다."],"id":"YNRsyJj2wH9L"},{"cell_type":"code","metadata":{"id":"check-env","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737763160084,"user_tz":-540,"elapsed":270,"user":{"displayName":"Jeonghwan Gwak","userId":"13669944636107147874"}},"outputId":"166e776a-350f-4cc0-f2f2-d90d50c56bf0"},"source":["import torch\n","import torchvision\n","\n","print(\"[INFO] PyTorch version:\", torch.__version__)\n","print(\"[INFO] TorchVision version:\", torchvision.__version__)\n","\n","# device 설정 (GPU가 사용 가능하면 GPU, 아니면 CPU)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"[INFO] Device:\", device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] PyTorch version: 2.5.1+cpu\n","[INFO] TorchVision version: 0.20.1+cpu\n","[INFO] Device: cpu\n"]}],"id":"check-env"},{"cell_type":"markdown","metadata":{"id":"mQyj7i5WwH9M"},"source":["## 2. 데이터 다운로드\n","\n","[**pizza_steak_sushi**](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip) 데이터셋을 **download_data()** 함수를 사용해 내려받습니다.\n","\n","압축이 풀리면 구조는 다음과 같습니다:\n","\n","```\n","pizza_steak_sushi/\n","├── train/\n","│   ├── pizza/\n","│   ├── steak/\n","│   └── sushi/\n","└── test/\n","    ├── pizza/\n","    ├── steak/\n","    └── sushi/\n","```\n"],"id":"mQyj7i5WwH9M"},{"cell_type":"code","metadata":{"id":"download-data","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737763166261,"user_tz":-540,"elapsed":837,"user":{"displayName":"Jeonghwan Gwak","userId":"13669944636107147874"}},"outputId":"2ca5b97d-260b-488d-afc4-43a8ece37be6"},"source":["from helper_functions import download_data\n","image_path = download_data(\n","    source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n","    destination=\"pizza_steak_sushi\"  # 이 폴더명으로 다운로드 & 압축해제\n",")\n","print(\"[INFO] Data downloaded to:\", image_path)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Did not find data/pizza_steak_sushi directory, creating one...\n","[INFO] Downloading pizza_steak_sushi.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip...\n","[INFO] Unzipping pizza_steak_sushi.zip data...\n","[INFO] Data downloaded to: data/pizza_steak_sushi\n"]}],"id":"download-data"},{"cell_type":"markdown","metadata":{"id":"1ygVK4nkwH9M"},"source":["## 3. 데이터셋 / 데이터로더 생성\n","\n","파이토치의 [**ImageFolder**](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html) 클래스를 사용해 이미지 폴더 구조를 **Dataset**으로 만들고, 이를 **DataLoader**로 감싸 학습에 활용합니다.\n","\n","- **변환(Transform)**: 이미지를 $(224, 224)$ 사이즈로 리사이즈, 텐서 형태로 변환\n","- **배치 크기**: 32\n","- **shuffle=True** (학습용)\n"],"id":"1ygVK4nkwH9M"},{"cell_type":"code","metadata":{"id":"data-setup","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737763169523,"user_tz":-540,"elapsed":257,"user":{"displayName":"Jeonghwan Gwak","userId":"13669944636107147874"}},"outputId":"092f30f1-a10b-4a11-fb75-cdf1f8ead13b"},"source":["import os\n","import torchvision\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","\n","# 이미지 변환 정의\n","#  - Resize(224,224): 이미지를 224x224로 맞춤\n","#  - ToTensor(): PyTorch 텐서([C,H,W])로 변환\n","manual_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor()\n","])\n","\n","# 학습/테스트 경로 설정\n","train_dir = image_path / \"train\"\n","test_dir = image_path / \"test\"\n","\n","# ImageFolder를 이용해 Dataset 만들기\n","train_dataset = ImageFolder(root=train_dir,\n","                            transform=manual_transforms)\n","test_dataset = ImageFolder(root=test_dir,\n","                           transform=manual_transforms)\n","\n","# Dataloader로 감싸기\n","train_loader = DataLoader(train_dataset,\n","                          batch_size=32,\n","                          shuffle=True)\n","test_loader = DataLoader(test_dataset,\n","                         batch_size=32,\n","                         shuffle=False)\n","\n","# 클래스 이름(폴더명) 확인\n","class_names = train_dataset.classes  # 예: ['pizza', 'steak', 'sushi']\n","print(\"[INFO] Class Names:\", class_names)\n","print(f\"[INFO] Total classes: {len(class_names)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Class Names: ['pizza', 'steak', 'sushi']\n","[INFO] Total classes: 3\n"]}],"id":"data-setup"},{"cell_type":"markdown","metadata":{"id":"CJERHuKqwH9N"},"source":["## 4. Vision Transformer 구현\n","\n","아래 단계로 **ViT**를 구성합니다:\n","\n","1. **Patch Embedding**:\n","   - 입력 이미지 텐서: $[B, C, H, W]$\n","   - $H, W$가 $\\text{patch_size}$로 나누어떨어진다고 가정\n","   - Conv2d(kernel_size=stride=$\\text{patch_size}$)로 $(H/\\text{patch_size}) \\times (W/\\text{patch_size})$ 패치를 임베딩 벡터($\\text{embedding_dim}$)로 변환\n","   - 최종 shape: $[B, N, D]$ (여기서 $N$은 패치 총 개수, $D$는 임베딩 차원)\n","\n","2. **Class Token / Position Embedding**:\n","   - Class 토큰(class token)이라는 학습 가능한 벡터를 맨 앞에 붙임\n","   - 위치 임베딩(position embedding)도 더해줌\n","   - 최종 shape: $[B, N+1, D]$\n","\n","3. **TransformerEncoderBlock** (Multi-Head Self Attention + MLP):\n","   - MultiheadSelfAttentionBlock\n","   - MLPBlock\n","   - 각 블록 사이에 residual 연결\n","\n","4. **최종 분류**:\n","   - Transformer 출력 중 **첫 번째 토큰(class token)**만 꺼내서 Linear로 분류\n","\n","이 과정을 PyTorch 코드로 하나씩 나눠서 보겠습니다."],"id":"CJERHuKqwH9N"},{"cell_type":"code","metadata":{"id":"vit-implementation"},"source":["import torch.nn as nn\n","\n","class PatchEmbedding(nn.Module):\n","    \"\"\"\n","    (1) 이미지(크기 HxW)를 (patch_size x patch_size) 크기로 나눈 뒤,\n","    각 패치를 embedding_dim 채널로 변환해주는 레이어.\n","    \"\"\"\n","    def __init__(\n","        self,\n","        in_channels: int = 3,      # 입력 채널 수(RGB=3)\n","        patch_size: int = 16,     # 패치 한 변의 크기\n","        embedding_dim: int = 768  # 패치 임베딩 후 차원\n","    ):\n","        super().__init__()\n","\n","        # Conv2d를 이용해서 (patch_size x patch_size) 단위로 이미지를 쪼개면서\n","        # out_channels=embedding_dim 형태로 만듦.\n","        # kernel_size=stride=patch_size 이므로, 가로/세로 방향으로 패치가 딱 나누어짐.\n","        self.patcher = nn.Conv2d(\n","            in_channels=in_channels,\n","            out_channels=embedding_dim,\n","            kernel_size=patch_size,\n","            stride=patch_size,\n","            padding=0\n","        )\n","\n","        # 2D (h_patch, w_patch)를 1D로 펼쳐서 [batch, embedding_dim, num_patches] 형태로 만들기\n","        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n","        self.patch_size = patch_size\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # x.shape = [batch, in_channels, height, width]\n","        # Conv2d 적용 -> [batch, embedding_dim, h_patch, w_patch]\n","        x = self.patcher(x)\n","\n","        # Flatten -> [batch, embedding_dim, num_patches]\n","        x = self.flatten(x)\n","\n","        # permute -> [batch, num_patches, embedding_dim]\n","        x = x.permute(0, 2, 1)\n","        return x\n","\n","class MultiheadSelfAttentionBlock(nn.Module):\n","    \"\"\"\n","    (2) Multi-Head Self Attention + LayerNorm\n","    - residual은 TransformerEncoderBlock에서\n","    \"\"\"\n","    def __init__(\n","        self,\n","        embedding_dim: int = 768,\n","        num_heads: int = 12,\n","        attn_dropout: float = 0.0\n","    ):\n","        super().__init__()\n","        # LayerNorm: 마지막 차원(embedding_dim)에 대해 정규화\n","        self.layer_norm = nn.LayerNorm(embedding_dim)\n","\n","        # MultiheadAttention: query, key, value가 전부 같음(self-attention)\n","        self.multihead_attn = nn.MultiheadAttention(\n","            embed_dim=embedding_dim,\n","            num_heads=num_heads,\n","            dropout=attn_dropout,\n","            batch_first=True  # 입력 shape: (batch, seq_len, embed_dim)\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # LayerNorm\n","        x_normed = self.layer_norm(x)\n","\n","        # Self-Attention 수행 (Q=K=V=x_normed)\n","        attn_output, _ = self.multihead_attn(\n","            x_normed,\n","            x_normed,\n","            x_normed,\n","            need_weights=False  # 어텐션 가중치 반환 안 함\n","        )\n","        return attn_output\n","\n","class MLPBlock(nn.Module):\n","    \"\"\"\n","    (3) MLP + LayerNorm\n","    - Linear -> GELU -> Dropout -> Linear -> Dropout\n","    - residual은 TransformerEncoderBlock에서\n","    \"\"\"\n","    def __init__(\n","        self,\n","        embedding_dim: int = 768,\n","        mlp_size: int = 3072,\n","        dropout: float = 0.1\n","    ):\n","        super().__init__()\n","        # LayerNorm\n","        self.layer_norm = nn.LayerNorm(embedding_dim)\n","\n","        # MLP 구조\n","        self.mlp = nn.Sequential(\n","            nn.Linear(embedding_dim, mlp_size),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(mlp_size, embedding_dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # LayerNorm\n","        x_normed = self.layer_norm(x)\n","        # MLP 통과\n","        x_out = self.mlp(x_normed)\n","        return x_out\n","\n","class TransformerEncoderBlock(nn.Module):\n","    \"\"\"\n","    (4) Transformer 블록 하나\n","       - MSA 블록 -> residual\n","       - MLP 블록 -> residual\n","    \"\"\"\n","    def __init__(\n","        self,\n","        embedding_dim: int = 768, # 임베딩 차원\n","        num_heads: int = 12, # 어텐션 헤드의 수\n","        mlp_size: int = 3072, # MLP hidden layer의 크기\n","        mlp_dropout: float = 0.1, # MLP의 dropout 비율 (기본값: 0.1)\n","        attn_dropout: float = 0.0 # 어텐션의 dropout 비율 (기본값: 0.0)\n","    ):\n","        super().__init__() # 부모 클래스(nn.Module) 초기화\n","\n","         # 멀티헤드 셀프 어텐션 블록 초기화\n","        self.msa_block = MultiheadSelfAttentionBlock(\n","            embedding_dim=embedding_dim,\n","            num_heads=num_heads,\n","            attn_dropout=attn_dropout\n","        )\n","        # MLP 블록 초기화\n","        self.mlp_block = MLPBlock(\n","            embedding_dim=embedding_dim,\n","            mlp_size=mlp_size,\n","            dropout=mlp_dropout\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # (A) MSA 결과 + residual\n","        x = self.msa_block(x) + x\n","        # (B) MLP 결과 + residual\n","        x = self.mlp_block(x) + x\n","        return x\n","\n","class ViT(nn.Module):\n","    \"\"\"\n","    (5) 최종 Vision Transformer\n","        - PatchEmbedding\n","        - class 토큰, position 임베딩 추가\n","        - 여러 TransformerEncoderBlock 쌓기\n","        - 첫 번째 토큰(class token)만 꺼내서 최종 Linear 분류\n","    \"\"\"\n","    def __init__(\n","        self,\n","        img_size: int = 224,\n","        in_channels: int = 3,\n","        patch_size: int = 16,\n","        num_transformer_layers: int = 12,\n","        embedding_dim: int = 768,\n","        mlp_size: int = 3072,\n","        num_heads: int = 12,\n","        attn_dropout: float = 0.0,\n","        mlp_dropout: float = 0.1,\n","        embedding_dropout: float = 0.1,\n","        num_classes: int = 3  # pizza, steak, sushi (3개)\n","    ):\n","        super().__init__()\n","\n","        # 패치 개수: (224 x 224) / (16 x 16) = 196\n","        self.num_patches = (img_size * img_size) // (patch_size**2)\n","\n","        # class 토큰(학습 가능한 파라미터)\n","        # shape: (1, 1, embedding_dim)\n","        self.class_embedding = nn.Parameter(\n","            torch.randn(1, 1, embedding_dim)\n","        )\n","\n","        # 위치 임베딩(학습 가능), 패치 개수+1 만큼\n","        self.position_embedding = nn.Parameter(\n","            torch.randn(1, self.num_patches + 1, embedding_dim)\n","        )\n","\n","        # 패치 임베딩 후에 쓸 드롭아웃\n","        self.embedding_dropout = nn.Dropout(embedding_dropout)\n","\n","        # (1) Patch Embedding 레이어\n","        self.patch_embedding = PatchEmbedding(\n","            in_channels=in_channels,\n","            patch_size=patch_size,\n","            embedding_dim=embedding_dim\n","        )\n","\n","        # (2) Transformer 인코더 블록을 num_transformer_layers개 쌓기\n","        self.transformer_encoder = nn.Sequential(\n","            *[\n","                TransformerEncoderBlock(\n","                    embedding_dim=embedding_dim,\n","                    num_heads=num_heads,\n","                    mlp_size=mlp_size,\n","                    mlp_dropout=mlp_dropout,\n","                    attn_dropout=attn_dropout\n","                ) for _ in range(num_transformer_layers)\n","            ]\n","        )\n","\n","        # (3) 최종 분류 레이어: LayerNorm -> Linear\n","        self.classifier = nn.Sequential(\n","            nn.LayerNorm(embedding_dim),\n","            nn.Linear(embedding_dim, num_classes)\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        batch_size = x.shape[0]\n","\n","        # class 토큰을 batch 크기만큼 확장: shape (batch, 1, embedding_dim)\n","        class_token = self.class_embedding.expand(batch_size, -1, -1)\n","\n","        # (A) Patch Embedding -> [batch, num_patches, embedding_dim]\n","        x = self.patch_embedding(x)\n","\n","        # (B) class 토큰을 맨 앞에 붙이기: [batch, num_patches+1, embedding_dim]\n","        x = torch.cat((class_token, x), dim=1)\n","\n","        # (C) 위치 임베딩 더하기\n","        x = x + self.position_embedding\n","        # 드롭아웃 적용\n","        x = self.embedding_dropout(x)\n","\n","        # (D) Transformer 인코더 여러 블록 통과\n","        x = self.transformer_encoder(x)\n","        # x.shape = [batch, num_patches+1, embedding_dim]\n","\n","        # (E) 첫 번째 토큰(class 토큰)에 해당하는 벡터만 뽑아서 분류\n","        x = self.classifier(x[:, 0])\n","        # x.shape = [batch, num_classes]\n","        return x"],"execution_count":null,"outputs":[],"id":"vit-implementation"},{"cell_type":"markdown","metadata":{"id":"mxHvV42UwH9O"},"source":["## 5. 학습 루프 (Training Loop)\n","\n","다음은 **기본적인 PyTorch 학습 루프** 예시입니다.\n","\n","1. 모델을 `model.train()` 모드로 설정\n","2. Dataloader에서 배치 단위로 `(images, labels)`를 가져옴\n","3. 순전파(forward) → 손실(loss) 계산 → 역전파(backprop) → 가중치 업데이트\n","4. 에폭이 끝나면, `model.eval()` 모드로 전환하여 테스트 세트 성능 측정\n","\n","위 과정을 원하는 만큼(`num_epochs`) 반복합니다."],"id":"mxHvV42UwH9O"},{"cell_type":"code","metadata":{"id":"training-loop"},"source":["def train_model(\n","    model: nn.Module,\n","    train_dataloader: DataLoader,\n","    test_dataloader: DataLoader,\n","    num_epochs: int = 5,\n","    lr: float = 1e-3\n","):\n","    \"\"\"\n","    단순한 학습 함수:\n","    - num_epochs만큼 학습을 반복\n","    - 매 epoch마다 Train Loss/Acc, Test Loss/Acc 출력\n","    \"\"\"\n","    model.to(device)  # 모델을 GPU/CPU로 이동\n","\n","    # 분류 문제 -> CrossEntropyLoss\n","    criterion = nn.CrossEntropyLoss()\n","    # 최적화 알고리즘 -> Adam\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(num_epochs):\n","        # -------------------------------\n","        # (A) 학습 모드\n","        # -------------------------------\n","        model.train()\n","        train_loss = 0.0\n","        train_correct = 0\n","        train_total = 0\n","\n","        for images, labels in train_dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            # 1) 순전파\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            # 2) 역전파\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # 통계 계산(정확도 등)\n","            _, preds = torch.max(outputs, dim=1)\n","            train_loss += loss.item() * images.size(0)\n","            train_correct += torch.sum(preds == labels).item()\n","            train_total += images.size(0)\n","\n","        epoch_train_loss = train_loss / train_total\n","        epoch_train_acc = train_correct / train_total\n","\n","        # -------------------------------\n","        # (B) 평가(테스트) 모드\n","        # -------------------------------\n","        model.eval()\n","        test_loss = 0.0\n","        test_correct = 0\n","        test_total = 0\n","\n","        with torch.inference_mode():\n","            for images, labels in test_dataloader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","                _, preds = torch.max(outputs, dim=1)\n","                test_loss += loss.item() * images.size(0)\n","                test_correct += torch.sum(preds == labels).item()\n","                test_total += images.size(0)\n","\n","        epoch_test_loss = test_loss / test_total\n","        epoch_test_acc = test_correct / test_total\n","\n","        # 결과 출력\n","        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n","              f\"Train Loss: {epoch_train_loss:.4f}, \"\n","              f\"Train Acc: {epoch_train_acc:.4f} | \"\n","              f\"Test Loss: {epoch_test_loss:.4f}, \"\n","              f\"Test Acc: {epoch_test_acc:.4f}\")"],"execution_count":null,"outputs":[],"id":"training-loop"},{"cell_type":"markdown","metadata":{"id":"Ge4zKixQwH9O"},"source":["## 6. 모델 생성 및 학습 실행\n","\n","마지막으로, **ViT 모델**을 생성하고, 앞서 만든 `train_model()` 함수로 학습을 수행합니다.\n","\n","- 기본 파라미터: $\\text{num_transformer_layers} = 12$, $\\text{embedding_dim} = 768$, 등\n","- 에폭 수(`num_epochs`)과 학습률(`lr`)은 자유롭게 조정하세요."],"id":"Ge4zKixQwH9O"},{"cell_type":"code","metadata":{"id":"training-run","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737764927628,"user_tz":-540,"elapsed":1736893,"user":{"displayName":"Jeonghwan Gwak","userId":"13669944636107147874"}},"outputId":"fb3c2c9c-e76a-4ae8-e13c-90dde2ef771a"},"source":["# 클래스 개수는 pizza, steak, sushi => 3개\n","num_classes = len(class_names)\n","\n","# ViT 모델 생성\n","model = ViT(\n","    num_classes=num_classes  # 분류 대상 = 3\n",")\n","\n","# 학습 실행 (5에폭, lr=1e-3)\n","train_model(\n","    model=model,\n","    train_dataloader=train_loader,\n","    test_dataloader=test_loader,\n","    num_epochs=5,\n","    lr=1e-3\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/5] Train Loss: 3.2139, Train Acc: 0.3111 | Test Loss: 1.3107, Test Acc: 0.3333\n","Epoch [2/5] Train Loss: 1.3715, Train Acc: 0.2978 | Test Loss: 1.1559, Test Acc: 0.2533\n","Epoch [3/5] Train Loss: 1.1686, Train Acc: 0.3333 | Test Loss: 1.1145, Test Acc: 0.3333\n","Epoch [4/5] Train Loss: 1.1448, Train Acc: 0.3067 | Test Loss: 1.1418, Test Acc: 0.2533\n","Epoch [5/5] Train Loss: 1.1668, Train Acc: 0.3333 | Test Loss: 1.1105, Test Acc: 0.4133\n"]}],"id":"training-run"}]}